# word2vec  
BowとWord2vecを用いたCBowとSkip-gramの実装  

# この技術の背景  
- コンピュータに自然言語を処理させるために単語を数値化(ベクトル化)する必要があり、このベクトルを分散表現と呼ぶ  
- 数値に変換する際は可能な限り元の単語の復元が可能であること  
- ベクトルの次元数が大きいほど計算コストが高くなるが、代わりに変換前の単語の表現力が高い  

# Bow  
- BoW: Bag of Wordsは単語の出現頻度に着目して分散表現を作成する方法(単語が出現していれば1、そうでなければ0)  
- 分散表現を得る基本的な手法だが、単語数に比例してベクトルの次元数が増えるため計算量が大きくなる  
- また、単語の前後関係や共起によって分散表現を作成するため単語のニュアンスの違いまで考慮できていない  

# CBow  
- CBoW: Continuous Bag of Wordsはある文章の特定の1単語を隠し、文章から単語を推測するというタスクを解くニューラルネットワークから作成されるWord2Vec分散表現  
- BoWに比べ、0≤cosθ≤1⇔0°≤θ≤90°0≤cos⁡θ≤1⇔0°≤θ≤90° だった表現範囲が−1≤cosθ≤1⇔0°≤θ≤180°−1≤cos⁡θ≤1⇔0°≤θ≤180°に広がっている  

# Skip-gram  
- skip-gramはCBoWとは逆にある文章の特定の1単語に注目し、単語から文章内の周辺語を推測するというタスクを解くニューラルネットワークから作成されるWord2Vec分散表現  

# 評価  
- CBoWよりもskip-gramで生成された分散表現の方が単語の概念的により的確なものである場合が多い。ただし学習コストはskip-gramの方が高くなる  
![評価]("C:\Users\KITM7438\Word2vec\スクリーンショット 2021-07-30 172205.png")  
